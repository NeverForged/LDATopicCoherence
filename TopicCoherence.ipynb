{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Coherence\n",
    "Based on http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class TopicCoherence(object):\n",
    "    '''\n",
    "    Based on the information here: http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "    Calculating the topic coherence for LDA through sklearn, rather than through Gensim\n",
    "    \n",
    "    ATTRIBUTES\n",
    "    D: Gives the Document counts, with D(wi) on the diagonal and D(wi,wj) as i,j\n",
    "    p: Gives the probabilities, with p(wi) on the diagonal and the rest as p(wi,wj)\n",
    "    vocabulary: saved the vocabulary from the fit, so we know what word is what\n",
    "    UCI_score: UCI Score (wi,wj)\n",
    "    UMass_score: UMass Score(wi,wj)\n",
    "    \n",
    "    METHODS:\n",
    "    fit(vocabulary, documents): Creates the Document Counts and the probability\n",
    "        counts for the corpus and vectorizer used.\n",
    "        vocabulary: CountVectorizer.vocabulary_\n",
    "        documents: the raw documents used, based on what was put into\n",
    "                    the count vectorizer.\n",
    "     \n",
    "    \n",
    "    Darin LaSota, 2/7/2019\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,words=10,score='both'):\n",
    "        '''\n",
    "        Initializer.\n",
    "        '''\n",
    "        self.words_to_use = words\n",
    "        self.score = score\n",
    "\n",
    "    def fit(self, vocabulary, transformed_docs, verbose=False, noise=True):\n",
    "        '''\n",
    "        This is to get the various document probabilities, likely because these will be the same \n",
    "        for all models if running this in a grid_search\n",
    "        '''\n",
    "        if noise == True:\n",
    "            noise = 0.000000000000000000000000000000000000000000001\n",
    "        else:\n",
    "            noise = 0\n",
    "        self.vocabulary = vocabulary  # save for later\n",
    "        self.docs_words = transformed_docs > 0\n",
    "        self.docs_words = self.docs_words*1.0\n",
    "        for i in range(transformed_docs.shape[0]):\n",
    "            for j in range(transformed_docs.shape[1]):\n",
    "                if transformed_docs[i,j]>0:\n",
    "                    self.docs_words[i,j] = 1\n",
    "        self.Di = np.sum(self.docs_words,0)\n",
    "        if verbose:\n",
    "            print('Di done')\n",
    "        self.Dij = np.zeros((self.Di.shape[1],self.Di.shape[1]))\n",
    "        for d in range(self.docs_words.shape[0]):\n",
    "            for i in range(self.docs_words.shape[1]):\n",
    "                for j in range(i+1,self.docs_words.shape[1]):\n",
    "                    self.Dij[i,j] = self.Dij[i,j] + self.docs_words[d,i]*self.docs_words[d,j]\n",
    "                    self.Dij[j,i] = self.Dij[j,i] + self.docs_words[d,i]*self.docs_words[d,j]\n",
    "        if verbose:\n",
    "            print('Dij done')       \n",
    "        self.pi = self.Di/transformed_docs.shape[0]\n",
    "        self.pij = self.Dij/transformed_docs.shape[0]\n",
    "        if verbose:\n",
    "            print('pi and pij done') \n",
    "        \n",
    "        if self.score == 'UCI' or self.score == 'both':\n",
    "            # UCI_score = log(p(wi,wj)/p(wi)p(wj))\n",
    "            self.UCI_score = np.zeros(self.Dij.shape)\n",
    "            for i in range(self.Dij.shape[0]):\n",
    "                for j in range(self.Dij.shape[1]):\n",
    "                    self.UCI_score[i,j] = np.log(noise+self.pij[i,j]/(self.pi[0,i]*self.pi[0,j]))\n",
    "            if verbose:\n",
    "                print('UCI_score done') \n",
    "        \n",
    "        if self.score == 'Umass' or self.score == 'both':\n",
    "            # Umass Score = log D(wi,wj) + 1 / D(wi)\n",
    "            self.UMass_score = np.zeros(self.Dij.shape)\n",
    "            for i in range(self.Dij.shape[0]):\n",
    "                for j in range(self.Dij.shape[1]):\n",
    "                    self.UMass_score[i,j] = np.log(1+self.Dij[i,j]/(self.Di[0,i]))\n",
    "            if verbose:\n",
    "                print('UMASS_score done')\n",
    "                \n",
    "    def tc_score(self,model,X,y=[]):\n",
    "        score = []\n",
    "        for topic_weights in model.components_:\n",
    "            top_keyword_locs = (-topic_weights).argsort()[:self.words_to_use]\n",
    "            for word in top_keyword_locs:\n",
    "                if self.score == 'UCI' or self.score == 'both':\n",
    "                    uci = np.mean([self.UCI_score[word,a] for a in topic_weights if a != word])\n",
    "                if self.score == 'UMass' or self.score == 'both':\n",
    "                    umass = np.mean([self.UMass_score[word,a] for a in topic_weights if a != word])\n",
    "                if self.score == 'Umass':\n",
    "                    score.append(umass)\n",
    "                elif self.score == 'UCI':\n",
    "                    score.append(uci)\n",
    "                else:\n",
    "                    score.append(uci*umass)\n",
    "\n",
    "        return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 65012)\n"
     ]
    }
   ],
   "source": [
    "# Data import on its own line\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import re, nltk, gensim, spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=1,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "count_vector = data_vectorized\n",
    "print(count_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TopicCoherence = TopicCoherence()\n",
    "TopicCoherence.fit(vectorizer.vocabulary, data_vectorized,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "params = {'n_components':[10,15,20,25,30] \n",
    "         , 'learning_decay':[0.5,0.7,0.9]}\n",
    "model =  LatentDirichletAllocation()\n",
    "gs = GridSearchCV(model,params,cv=5,scoring=TopicCoherence.tc_score)\n",
    "gs.fit(count_vector)\n",
    "\n",
    "plot_grid_search(gs.cv_results_, params['n_components'], params['learning_decay'], 'Components', 'Learning Decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "print(gs.best_params_)\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(gs.best_estimator_, data_vectorized, vectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
